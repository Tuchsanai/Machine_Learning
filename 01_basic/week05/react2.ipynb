{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Understanding ReAct (🤖 Reason + ⚡ Act) with OpenAI\n",
    "\n",
    "Welcome! In this lab, we'll explore the **ReAct** prompting technique. ReAct is a powerful pattern that enables Large Language Models (LLMs) to solve complex problems by combining reasoning and action steps.\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "Instead of just asking an LLM for a final answer, we ask it to \"think out loud.\" It breaks down a problem into a sequence of steps:\n",
    "\n",
    "1.  **Thought 🤔:** The model reasons about the problem and decides what to do next.\n",
    "2.  **Action ⚡:** The model chooses a tool or action to perform to gather information.\n",
    "3.  **Observation 👀:** The result of the action is given back to the model.\n",
    "\n",
    "This loop repeats until the model has enough information to give a final answer. It makes the model's process more transparent and often more accurate, especially for questions that require up-to-date information or calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary library and set up our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# It's best practice to use environment variables for API keys.\n",
    "# If you don't have it set, this will prompt you to enter it securely.\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"✅ OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Prompt vs. ReAct Prompt\n",
    "\n",
    "Let's start with a simple question and see the difference between a standard prompt and a ReAct-style prompt.\n",
    "\n",
    "**Question:** *\"Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Prompt\n",
    "\n",
    "Here, we just ask the question directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_prompt = \"Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": standard_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct-style Prompt\n",
    "\n",
    "Now, let's instruct the model to use the ReAct format. We are not using external tools yet, just asking it to show its reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_prompt = \"\"\"\n",
    "Answer the following question by reasoning step-by-step. Use the following format:\n",
    "\n",
    "Thought: Your reasoning about the question.\n",
    "Final Answer: The final answer to the user's question.\n",
    "\n",
    "Question: Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": react_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Notice how the ReAct prompt forces the model to lay out its thinking process. This isn't super useful yet because the model is just using its internal knowledge. The real power comes when we give it **tools** to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReAct with Tools (Manual Method)\n",
    "\n",
    "Now for the fun part! We'll create a simulated environment where the LLM can use external tools. We will define two Python functions that our model can learn to call:\n",
    "\n",
    "1.  `search(query)`: Simulates a web search.\n",
    "2.  `calculate(expression)`: Simulates a calculator.\n",
    "\n",
    "To handle all the new problems, we'll expand the hardcoded knowledge base in our `search` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded knowledge base for our simulated search engine\n",
    "KNOWLEDGE_BASE = {\n",
    "    \"when was the first iphone released\": \"The first iPhone was released on June 29, 2007.\",\n",
    "    \"us president in 2007\": \"George W. Bush was the US president in 2007. He was born on July 6, 1946.\",\n",
    "    \"first iphone release date\": \"The first iPhone was released on June 29, 2007.\",\n",
    "    \"george w. bush birthdate\": \"George W. Bush was born on July 6, 1946.\",\n",
    "    \"us president june 29, 2007\": \"George W. Bush was the US president in 2007. He was born on July 6, 1946.\",\n",
    "    \"when did man land on the moon\": \"The first moon landing was on July 20, 1969.\",\n",
    "    \"uk monarch in 1969\": \"Queen Elizabeth II was the monarch of the United Kingdom in 1969. She was born on April 21, 1926.\",\n",
    "    \"queen elizabeth ii birthdate\": \"Queen Elizabeth II was born on April 21, 1926.\",\n",
    "    \"who composed the four seasons\": \"Antonio Vivaldi composed 'The Four Seasons'. He died in 1741.\",\n",
    "    \"antonio vivaldi death year\": \"Antonio Vivaldi died in 1741.\",\n",
    "    \"earth orbital speed\": \"The Earth's average orbital speed is approximately 107,000 kilometers per hour.\",\n",
    "    \"capital of australia\": \"The capital of Australia is Canberra.\",\n",
    "    \"population of canberra\": \"As of 2023, the population of Canberra is approximately 467,000.\",\n",
    "    \"boiling point of water celsius\": \"The boiling point of water at sea level is 100 degrees Celsius.\",\n",
    "    \"fifa world cup 2014 winner\": \"Germany won the FIFA World Cup in 2014. The runner-up was Argentina.\",\n",
    "    \"main ingredient margarita cocktail\": \"The main ingredient in a Margarita cocktail is tequila.\"\n",
    "}\n",
    "\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Simulates searching the web for a given query.\"\"\"\n",
    "    print(f\"\\n🤖 SEARCHING for: '{query}'...\")\n",
    "    query = query.lower()\n",
    "    return KNOWLEDGE_BASE.get(query, \"Sorry, I couldn't find any information on that.\")\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Simulates a calculator that can evaluate a simple math expression.\"\"\"\n",
    "    print(f\"\\n🧮 CALCULATING: '{expression}'...\")\n",
    "    try:\n",
    "        # A safer way to evaluate a string expression\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result is {result}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test our tools\n",
    "print(search(\"uk monarch in 1969\"))\n",
    "print(calculate(\"2024 - 1741\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Manual ReAct Loop\n",
    "\n",
    "Now, we'll solve a multi-step problem. We will act as the \"computer\" that executes the actions proposed by the LLM. We will feed the observations back to the model until it finds the final answer.\n",
    "\n",
    "**Our complex question:** *\"Who was the president of the United States when the first iPhone was released, and what was his age at that time?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This is the core prompt that tells the LLM how to behave\n",
    "react_system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions by breaking them down into a series of thoughts and actions. Your goal is to find the final answer.\n",
    "\n",
    "You have access to the following tools:\n",
    "1. search(query: str): Use this to find information about current events, facts, and people.\n",
    "2. calculate(expression: str): Use this to perform mathematical calculations.\n",
    "\n",
    "Follow this format strictly:\n",
    "Thought: Your reasoning about what to do next to get closer to the answer.\n",
    "Action: The tool you want to use, in the format `search(\"your query\")` or `calculate(\"your expression\")`.\n",
    "\n",
    "After an action, you will receive an Observation. You will then repeat the Thought/Action cycle. Once you have enough information, provide the final answer in this format:\n",
    "Final Answer: [Your conclusive answer here]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_manual_react_loop(question: str, max_steps=5):\n",
    "    \"\"\"Runs the manual ReAct loop for a given question.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": react_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        print(f\"\\n--- STEP {i+1} ---\")\n",
    "\n",
    "        print(\"🤔 Thinking...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            stop=[\"\\nObservation:\"] # Stop generation when it's time for us to provide an observation\n",
    "        )\n",
    "        llm_output = response.choices[0].message.content\n",
    "        print(llm_output)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_output})\n",
    "\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            print(\"\\n✅ ReAct loop finished!\")\n",
    "            break\n",
    "\n",
    "        action_str = llm_output.split(\"Action:\")[-1].strip()\n",
    "        observation = \"\"\n",
    "        try:\n",
    "            if action_str.startswith(\"search(\"):\n",
    "                query = action_str[len(\"search(\"):-1].strip('\"')\n",
    "                observation = search(query)\n",
    "            elif action_str.startswith(\"calculate(\"):\n",
    "                expression = action_str[len(\"calculate(\"):-1].strip('\"')\n",
    "                observation = calculate(expression)\n",
    "            else:\n",
    "                observation = \"Error: Invalid action specified.\"\n",
    "        except Exception as e:\n",
    "             observation = f\"Error executing action: {e}\"\n",
    "\n",
    "        print(f\"👀 OBSERVATION: {observation}\")\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
    "    else:\n",
    "        print(\"\\n⚠️ ReAct loop reached max steps.\")\n",
    "\n",
    "# Let's run it with the original question!\n",
    "original_question = \"Who was the president of the United States when the first iPhone was released, and what was his age at that time?\"\n",
    "run_manual_react_loop(original_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced ReAct: Automated Tool Calling\n",
    "\n",
    "The manual method is great for understanding the ReAct concept, but it's brittle. It relies on the model generating text in a *perfect* format that we have to parse. \n",
    "\n",
    "Modern LLM APIs (like OpenAI's) have a much better way: **structured tool calling**. \n",
    "\n",
    "Here's how it works:\n",
    "1.  **Define Tools:** We describe our Python functions to the model in a structured format (JSON Schema).\n",
    "2.  **API Call:** We send the conversation history and the list of tools to the model.\n",
    "3.  **Model Response:** If the model decides to use a tool, it doesn't just write text. It returns a `tool_calls` object containing the exact function name and arguments it wants to use.\n",
    "4.  **Execute & Respond:** Our code executes the specified function with the given arguments and sends the result back to the model in a new turn. \n",
    "\n",
    "This is more reliable, secure, and the standard for building modern AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Define tools in the format the OpenAI API expects.\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"Searches the web for information on various topics, including dates, facts, and people.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query (e.g., 'when did man land on the moon')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Performs mathematical calculations.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate (e.g., '1969 - 1926')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Map tool names to actual functions\n",
    "available_functions = {\n",
    "    \"search\": search,\n",
    "    \"calculate\": calculate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_tool_calling_react_loop(question: str, max_steps=5):\n",
    "    \"\"\"Runs the ReAct loop for a given question using structured tool calling.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        print(f\"\\n--- STEP {i+1} ---\")\n",
    "        print(\"🤔 Thinking...\")\n",
    "\n",
    "        # The core API call, now with tools!\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\", # The model decides when to call tools\n",
    "        )\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        tool_calls = response_message.tool_calls\n",
    "\n",
    "        # 2. Check if the model wants to call a tool\n",
    "        if tool_calls:\n",
    "            print(f\"⚡ Action: Decided to call {len(tool_calls)} tool(s).\")\n",
    "            messages.append(response_message) # Append the assistant's message with tool calls\n",
    "\n",
    "            # 3. Execute the tools\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                # Call the actual Python function\n",
    "                function_response = function_to_call(**function_args)\n",
    "                \n",
    "                print(f\"👀 OBSERVATION (from {function_name}): {function_response}\")\n",
    "                \n",
    "                # 4. Append the tool's output to the conversation\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_response,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            # No tool call, so it must be the final answer\n",
    "            final_answer = response_message.content\n",
    "            print(f\"\\n✅ Final Answer: {final_answer}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"\\n⚠️ ReAct loop reached max steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling - Problem 1\n",
    "\n",
    "**Question:** *\"Who was the monarch of the United Kingdom when the first person landed on the moon, and how old were they at that time?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_1 = \"Who was the monarch of the United Kingdom when the first person landed on the moon, and how old were they at that time?\"\n",
    "run_tool_calling_react_loop(problem_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling - Problem 2\n",
    "\n",
    "**Question:** *\"Who composed 'The Four Seasons', and roughly how many years passed between his death and the release of the first iPhone?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_2 = \"Who composed 'The Four Seasons', and roughly how many years passed between his death and the release of the first iPhone?\"\n",
    "run_tool_calling_react_loop(problem_2, max_steps=6) # Allow more steps if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling - Problem 3\n",
    "\n",
    "**Question:** *\"What is the Earth's average orbital speed in kilometers per hour?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_3 = \"What is the Earth's average orbital speed in kilometers per hour?\"\n",
    "run_tool_calling_react_loop(problem_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Practice (Manual ReAct Loop)\n",
    "\n",
    "Now, try solving these remaining problems using the **original manual ReAct loop** (`run_manual_react_loop`). This will help reinforce the fundamental `Thought -> Action -> Observation` cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Loop - Problem 4\n",
    "\n",
    "**Question:** *\"What is the capital of Australia and what is its population?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_4 = \"What is the capital of Australia and what is its population?\"\n",
    "run_manual_react_loop(problem_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Loop - Problem 5\n",
    "\n",
    "**Question:** *\"How do I convert the boiling point of water from Celsius to Fahrenheit? The formula is (C * 9/5) + 32.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_5 = \"First, find the boiling point of water in Celsius. Then, calculate the equivalent in Fahrenheit using the formula (C * 9/5) + 32.\"\n",
    "run_manual_react_loop(problem_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Challenges\n",
    "\n",
    "Here are more questions you can try to solve on your own by calling the `run_manual_react_loop` function. \n",
    "\n",
    "6. **FIFA World Cup:** *\"Who won the FIFA World Cup in 2014, and who was the runner-up?\"*\n",
    "7. **Movie Time:** *\"If a movie is 148 minutes long, how long is it in hours and minutes?\"* (Hint: this requires calculation, specifically division and modulo)\n",
    "8. **Simple Interest:** *\"If I invest $5,000 at an annual interest rate of 7% for 10 years (simple interest), what will my total interest earnings be?\"* (Formula: Principal * Rate * Time)\n",
    "9. **Cocktails:** *\"What is the main ingredient in a Margarita cocktail?\"*\n",
    "10. **Literature:** *\"Who wrote 'The Hitchhiker's Guide to the Galaxy' and when was it first published?\"* (Note: You may need to add this info to the `KNOWLEDGE_BASE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Key Takeaways\n",
    "\n",
    "Congratulations on completing the expanded ReAct lab! 🥳\n",
    "\n",
    "You've successfully guided an LLM to solve complex problems using both a manual ReAct loop and the more modern, structured tool-calling approach.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1.  **Transparency:** The `Thought` process shows you *how* the model is working, making it easier to debug and trust.\n",
    "2.  **Accuracy:** By using external tools (like search or a calculator), the model can access up-to-date information and perform precise calculations, overcoming its inherent limitations.\n",
    "3.  **Extensibility:** You can create any tool you want! Imagine giving the model tools to check your calendar, send an email, or query a database.\n",
    "4.  **Evolution of ReAct:** You've seen the progression from parsing raw text (the manual loop) to using structured **tool calling**. The latter is far more robust and is the standard method used in modern agentic frameworks.\n",
    "\n",
    "**In the real world**, frameworks like **LangChain**, **LlamaIndex**, and direct SDKs for models like GPT-4o automate the tool-calling loop, making it much easier to build powerful, tool-augmented AI applications. Understanding the fundamental `Thought -> Action -> Observation` cycle is the key to mastering them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}