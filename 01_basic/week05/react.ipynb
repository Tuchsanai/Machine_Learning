{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Understanding ReAct (ðŸ¤– Reason + âš¡ Act) with OpenAI\n",
    "\n",
    "Welcome! In this lab, we'll explore the **ReAct** prompting technique. ReAct is a powerful pattern that enables Large Language Models (LLMs) to solve complex problems by combining reasoning and action steps.\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "Instead of just asking an LLM for a final answer, we ask it to \"think out loud.\" It breaks down a problem into a sequence of steps:\n",
    "\n",
    "1.  **Thought ðŸ¤”:** The model reasons about the problem and decides what to do next.\n",
    "2.  **Action âš¡:** The model chooses a tool or action to perform to gather information.\n",
    "3.  **Observation ðŸ‘€:** The result of the action is given back to the model.\n",
    "\n",
    "This loop repeats until the model has enough information to give a final answer. It makes the model's process more transparent and often more accurate, especially for questions that require up-to-date information or calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary library and set up our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from getpass import getpass # To securely ask for the API key\n",
    "\n",
    "# It's best practice to use environment variables for API keys.\n",
    "# If you don't have it set, this will prompt you to enter it securely.\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API Key: ')\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Prompt vs. ReAct Prompt\n",
    "\n",
    "Let's start with a simple question and see the difference between a standard prompt and a ReAct-style prompt.\n",
    "\n",
    "**Question:** *\"Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Prompt\n",
    "\n",
    "Here, we just ask the question directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carlos Alcaraz won the men's singles title at Wimbledon in 2023. He defeated Novak Djokovic in the final.\n"
     ]
    }
   ],
   "source": [
    "standard_prompt = \"Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": standard_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct-style Prompt\n",
    "\n",
    "Now, let's instruct the model to use the ReAct format. We are not using external tools yet, just asking it to show its reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user wants to know the winner and runner-up of the 2023 Wimbledon men's singles final. I need to access my internal knowledge about this event.\n",
      "\n",
      "Final Answer: The winner of the men's singles title at Wimbledon in 2023 was Carlos Alcaraz, and he beat Novak Djokovic in the final.\n"
     ]
    }
   ],
   "source": [
    "react_prompt = \"\"\"\n",
    "Answer the following question by reasoning step-by-step. Use the following format:\n",
    "\n",
    "Thought: The user wants to know the winner and runner-up of the 2023 Wimbledon men's singles final. I need to access my internal knowledge about this event.\n",
    "Final Answer: The final answer to the user's question.\n",
    "\n",
    "Question: Who won the men's singles title at Wimbledon in 2023 and who did he beat in the final?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": react_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Notice how the ReAct prompt forces the model to lay out its thinking process. This isn't super useful yet because the model is just using its internal knowledge. The real power comes when we give it **tools** to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReAct with Tools\n",
    "\n",
    "Now for the fun part! We'll create a simulated environment where the LLM can use external tools. We will define two simple Python functions:\n",
    "\n",
    "1.  `search(query)`: Simulates a web search.\n",
    "2.  `calculate(expression)`: Simulates a calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– SEARCHING for: 'US President in 2007'...\n",
      "George W. Bush was the US president in 2007. He was born on July 6, 1946.\n",
      "\n",
      "ðŸ§® CALCULATING: '2007 - 1946'...\n",
      "The result is 61.\n"
     ]
    }
   ],
   "source": [
    "# Tool 1: A simulated search engine\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Simulates searching the web for a given query.\"\"\"\n",
    "    print(f\"\\nðŸ¤– SEARCHING for: '{query}'...\")\n",
    "    # We'll hardcode the results for this lab\n",
    "    query = query.lower()\n",
    "    if \"when was the first iphone released\" in query:\n",
    "        return \"The first iPhone was released on June 29, 2007.\"\n",
    "    elif \"us president in 2007\" in query:\n",
    "        return \"George W. Bush was the US president in 2007. He was born on July 6, 1946.\"\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find any information on that.\"\n",
    "\n",
    "# Tool 2: A simulated calculator\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Simulates a calculator that can evaluate a simple math expression.\"\"\"\n",
    "    print(f\"\\nðŸ§® CALCULATING: '{expression}'...\")\n",
    "    try:\n",
    "        # A safe way to evaluate a string expression\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result is {result}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test our tools\n",
    "print(search(\"US President in 2007\"))\n",
    "print(calculate(\"2007 - 1946\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Manual ReAct Loop\n",
    "\n",
    "Now, we'll solve a multi-step problem. We will act as the \"computer\" that executes the actions proposed by the LLM. We will feed the observations back to the model until it finds the final answer.\n",
    "\n",
    "**Our complex question:** *\"Who was the president of the United States when the first iPhone was released, and what was his age at that time?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the core prompt that tells the LLM how to behave\n",
    "react_system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions by breaking them down into a series of thoughts and actions. Your goal is to find the final answer.\n",
    "\n",
    "You have access to the following tools:\n",
    "1. search(query: str): Use this to find information about current events, facts, and people.\n",
    "2. calculate(expression: str): Use this to perform mathematical calculations.\n",
    "\n",
    "Follow this format strictly:\n",
    "Thought: Your reasoning about what to do next to get closer to the answer.\n",
    "Action: The tool you want to use, in the format `search(\"your query\")` or `calculate(\"your expression\")`.\n",
    "\n",
    "After an action, you will receive an Observation. You will then repeat the Thought/Action cycle. Once you have enough information, provide the final answer in this format:\n",
    "Final Answer: [Your conclusive answer here]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1 ---\n",
      "ðŸ¤” Thinking...\n",
      "Thought: To find out who was the president of the United States when the first iPhone was released, I need to know the release date of the first iPhone. Then, I can determine who was the president at that time and calculate his age.\n",
      "\n",
      "Action: search(\"first iPhone release date\")\n",
      "\n",
      "ðŸ¤– SEARCHING for: 'first iPhone release date'...\n",
      "ðŸ‘€ OBSERVATION: Sorry, I couldn't find any information on that.\n",
      "\n",
      "--- STEP 2 ---\n",
      "ðŸ¤” Thinking...\n",
      "Thought: I know from prior knowledge that the first iPhone was released on June 29, 2007. Now, I need to find out who was the president of the United States on that date.\n",
      "\n",
      "Action: search(\"US president June 29, 2007\")\n",
      "\n",
      "ðŸ¤– SEARCHING for: 'US president June 29, 2007'...\n",
      "ðŸ‘€ OBSERVATION: Sorry, I couldn't find any information on that.\n",
      "\n",
      "--- STEP 3 ---\n",
      "ðŸ¤” Thinking...\n",
      "Thought: I know from prior knowledge that George W. Bush was the President of the United States in 2007. Now, I need to find out his birthdate to calculate his age on June 29, 2007.\n",
      "\n",
      "Action: search(\"George W. Bush birthdate\")\n",
      "\n",
      "ðŸ¤– SEARCHING for: 'George W. Bush birthdate'...\n",
      "ðŸ‘€ OBSERVATION: Sorry, I couldn't find any information on that.\n",
      "\n",
      "--- STEP 4 ---\n",
      "ðŸ¤” Thinking...\n",
      "Thought: I know from prior knowledge that George W. Bush was born on July 6, 1946. Now, I can calculate his age on June 29, 2007.\n",
      "\n",
      "Action: calculate(\"2007 - 1946\")\n",
      "\n",
      "ðŸ§® CALCULATING: '2007 - 1946'...\n",
      "ðŸ‘€ OBSERVATION: The result is 61.\n",
      "\n",
      "--- STEP 5 ---\n",
      "ðŸ¤” Thinking...\n",
      "Thought: Since George W. Bush was born on July 6, 1946, and the date in question is June 29, 2007, he had not yet reached his 61st birthday in 2007. Therefore, he was still 60 years old on June 29, 2007.\n",
      "\n",
      "Final Answer: The president of the United States when the first iPhone was released was George W. Bush, and he was 60 years old at that time.\n",
      "\n",
      "âœ… ReAct loop finished!\n"
     ]
    }
   ],
   "source": [
    "# We will store the conversation history here\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": react_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Who was the president of the United States when the first iPhone was released, and what was his age at that time?\"}\n",
    "]\n",
    "\n",
    "def run_react_loop(max_steps=5):\n",
    "    for i in range(max_steps):\n",
    "        print(f\"\\n--- STEP {i+1} ---\")\n",
    "\n",
    "        # 1. Get the LLM's next Thought and Action\n",
    "        print(\"ðŸ¤” Thinking...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            stop=[\"\\nObservation:\"] # Stop generation when it's time for us to provide an observation\n",
    "        )\n",
    "        llm_output = response.choices[0].message.content\n",
    "        print(llm_output)\n",
    "\n",
    "        # Add the LLM's turn to the conversation history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_output})\n",
    "\n",
    "        # 2. Check if the LLM provided a final answer\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            print(\"\\nâœ… ReAct loop finished!\")\n",
    "            break\n",
    "\n",
    "        # 3. Execute the Action\n",
    "        action_str = llm_output.split(\"Action:\")[-1].strip()\n",
    "        observation = \"\"\n",
    "        if action_str.startswith(\"search(\"):\n",
    "            query = action_str[len(\"search(\"):-1].strip('\"')\n",
    "            observation = search(query)\n",
    "        elif action_str.startswith(\"calculate(\"):\n",
    "            expression = action_str[len(\"calculate(\"):-1].strip('\"')\n",
    "            observation = calculate(expression)\n",
    "        else:\n",
    "            observation = \"Error: Invalid action specified.\"\n",
    "\n",
    "        # 4. Provide the Observation back to the LLM\n",
    "        print(f\"ðŸ‘€ OBSERVATION: {observation}\")\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
    "\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ ReAct loop reached max steps.\")\n",
    "\n",
    "# Let's run it!\n",
    "run_react_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion & Key Takeaways\n",
    "\n",
    "Congratulations on completing the ReAct lab! ðŸ¥³\n",
    "\n",
    "You've successfully guided an LLM to solve a complex problem by giving it tools and a reasoning framework. \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1.  **Transparency:** The `Thought` process shows you *how* the model is working, making it easier to debug and trust.\n",
    "2.  **Accuracy:** By using external tools (like search or a calculator), the model can access up-to-date information and perform precise calculations, overcoming its inherent limitations.\n",
    "3.  **Extensibility:** You can create any tool you want! Imagine giving the model tools to check your calendar, send an email, or query a database.\n",
    "\n",
    "**In the real world**, frameworks like **LangChain** and **LlamaIndex** automate this ReAct loop, making it much easier to build powerful, tool-augmented AI applications. However, understanding the fundamental `Thought -> Action -> Observation` cycle is the key to mastering them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
