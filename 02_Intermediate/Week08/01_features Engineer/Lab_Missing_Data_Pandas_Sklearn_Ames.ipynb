{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5082f853",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Beginner Lab: Handling Missing Data with pandas and scikit-learn (Ames Housing)\n",
    "\n",
    "**Goal:** Learn simple, practical ways to find and fix missing values using **pandas** and **scikit-learn**.\n",
    "We will use the dataset `Ames_outliers_removed.csv`. All code is beginner-friendly. No fancy tricks.\n",
    "\n",
    "### What you'll do\n",
    "1. Load the dataset and take a quick look.\n",
    "2. Measure missing values by column.\n",
    "3. Visualize which columns have the most missing data.\n",
    "4. Decide on basic strategies:\n",
    "   - Drop columns with too many missing values (optional).\n",
    "   - Drop rows only when truly necessary (optional).\n",
    "   - Fill numeric columns with **median**.\n",
    "   - Fill categorical columns with **most frequent** value.\n",
    "5. Save a clean, imputed CSV you can use later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a0989",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup\n",
    "\n",
    "Run the cell below to import libraries. If anything errors, install the missing package first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt  # We'll keep plots simple\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Make pandas show more columns so we can read tables easily\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb3c97",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load the dataset\n",
    "\n",
    "This notebook tries a few common locations. If it can't find the file, place `Ames_outliers_removed.csv`\n",
    "in the **same folder** as this notebook and run again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfe4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to find the dataset in a few common paths\n",
    "possible_paths = [\n",
    "    \"./Ames_outliers_removed.csv\",\n",
    "    \"Ames_outliers_removed.csv\",\n",
    "    \"/mnt/data/Ames_outliers_removed.csv\"  # Used in hosted environments\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        data_path = p\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find 'Ames_outliers_removed.csv'. \"\n",
    "        \"Place it in the same folder as this notebook and try again.\"\n",
    "    )\n",
    "\n",
    "print(\"Using file:\", data_path)\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224bf2c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick information about the data\n",
    "\n",
    "Use `info()` to see column types and a first sense of missingness (look at \"non-null\" counts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ce994",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Missing values overview by column\n",
    "\n",
    "We will count how many missing values each column has and also the percentage.\n",
    "Then we sort so the worst offenders are on top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count missing values per column\n",
    "missing_count = df.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "# Put into one easy-to-read table\n",
    "missing_table = (\n",
    "    pd.DataFrame({\"missing_count\": missing_count, \"missing_percent\": missing_percent})\n",
    "    .sort_values(by=\"missing_percent\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show the top 20 columns with missing values\n",
    "missing_table.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf664ea3",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Visualize missingness (Top 20)\n",
    "\n",
    "A simple bar chart helps you see which columns need attention first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_n = 20\n",
    "top_missing = missing_table.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_missing.index.astype(str), top_missing[\"missing_percent\"])\n",
    "plt.xticks(rotation=75, ha=\"right\")\n",
    "plt.ylabel(\"Percent missing\")\n",
    "plt.title(f\"Top {top_n} columns by missing percentage\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d58f4b",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Separate numeric and categorical columns\n",
    "\n",
    "We will impute them differently:\n",
    "- **Numeric:** use **median** (robust to outliers).\n",
    "- **Categorical:** use **most frequent** value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Work on a copy so we always keep the original df untouched\n",
    "df_work = df.copy()\n",
    "\n",
    "# Identify numeric and categorical columns in a simple way\n",
    "numeric_cols = df_work.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_cols = df_work.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", len(numeric_cols))\n",
    "print(\"Categorical columns:\", len(categorical_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed338de",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Optional: Drop columns with too many missing values\n",
    "\n",
    "A simple rule: if more than **40%** of a column is missing, it might be safer to drop it.\n",
    "This is not a law. It depends on your project. We just show how to do it.\n",
    "\n",
    "If you don't want to drop anything, set `threshold = 1.1` so nothing is dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47081fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a simple threshold\n",
    "threshold = 0.40  # 40%\n",
    "cols_to_drop = missing_table[missing_table[\"missing_percent\"] > (threshold * 100)].index.tolist()\n",
    "\n",
    "print(\"Columns suggested to drop (you can change the threshold):\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "# Make a version where those are dropped (optional)\n",
    "df_drop_cols = df_work.drop(columns=cols_to_drop)\n",
    "print(\"Shape before drop:\", df_work.shape, \" -> after drop:\", df_drop_cols.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2d9ce",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Optional: Drop rows in special cases\n",
    "\n",
    "Dropping rows is usually a last resort. Two common simple cases:\n",
    "1. Drop rows that are completely empty (rare).\n",
    "2. Drop rows missing a **critical** target column (for example, `SalePrice`).\n",
    "\n",
    "We will **not** drop rows by default here. The code below shows how you would do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: drop rows that are entirely missing (almost never needed)\n",
    "df_no_all_na_rows = df_drop_cols.dropna(how=\"all\")\n",
    "print(\"Rows before:\", len(df_drop_cols), \" -> after removing all-NA rows:\", len(df_no_all_na_rows))\n",
    "\n",
    "# Example: if you must have SalePrice for your task\n",
    "if \"SalePrice\" in df_no_all_na_rows.columns:\n",
    "    df_no_all_na_rows = df_no_all_na_rows.dropna(subset=[\"SalePrice\"])\n",
    "    print(\"After dropping rows missing SalePrice:\", len(df_no_all_na_rows))\n",
    "else:\n",
    "    print(\"Column 'SalePrice' not found. Skipping this specific drop example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61312c60",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Impute numeric columns (median)\n",
    "\n",
    "Why median? It is simple and resistant to outliers. This is a good default for beginners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f148a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take the current working DataFrame from the previous step\n",
    "df_impute_base = df_no_all_na_rows.copy()\n",
    "\n",
    "# Simple median imputer for numeric columns\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Fit on the numeric part and transform it\n",
    "numeric_imputed = num_imputer.fit_transform(df_impute_base[numeric_cols])\n",
    "\n",
    "# Put back into a DataFrame with the same column names and index\n",
    "numeric_imputed_df = pd.DataFrame(numeric_imputed, columns=numeric_cols, index=df_impute_base.index)\n",
    "\n",
    "# Peek at how many missing remain in numeric columns (should be zero)\n",
    "numeric_imputed_df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0846ce",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Impute categorical columns (most frequent)\n",
    "\n",
    "For text-like columns, the **most frequent** value is a very simple baseline that works okay to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01098ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If there are categorical columns, impute them\n",
    "if len(categorical_cols) > 0:\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    categorical_imputed = cat_imputer.fit_transform(df_impute_base[categorical_cols])\n",
    "    categorical_imputed_df = pd.DataFrame(categorical_imputed, columns=categorical_cols, index=df_impute_base.index)\n",
    "else:\n",
    "    # No categorical columns found\n",
    "    categorical_imputed_df = pd.DataFrame(index=df_impute_base.index)\n",
    "\n",
    "# Check missing in categorical part\n",
    "categorical_imputed_df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ea8b6",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Combine numeric and categorical back together\n",
    "\n",
    "Now we join the imputed numeric and categorical tables into one clean DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a742e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine along columns\n",
    "df_imputed = pd.concat([numeric_imputed_df, categorical_imputed_df], axis=1)\n",
    "\n",
    "# Keep original column order where possible\n",
    "# Any dropped columns will naturally be missing from this new DataFrame\n",
    "ordered_cols = [c for c in df_impute_base.columns if c in df_imputed.columns]\n",
    "df_imputed = df_imputed[ordered_cols]\n",
    "\n",
    "print(\"Shape after imputation:\", df_imputed.shape)\n",
    "df_imputed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c0456",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Check missing values before vs after\n",
    "\n",
    "We make a quick comparison to confirm our imputation worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf26bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total missing values in the original working frame (after any drops)\n",
    "before_total_na = df_impute_base.isna().sum().sum()\n",
    "\n",
    "# Total missing values after imputation\n",
    "after_total_na = df_imputed.isna().sum().sum()\n",
    "\n",
    "print(\"Total missing values before:\", before_total_na)\n",
    "print(\"Total missing values after :\", after_total_na)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47054a",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Save the cleaned data\n",
    "\n",
    "Save your imputed data to a new CSV. You can use it in future notebooks or models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"Ames_clean_imputed.csv\"\n",
    "df_imputed.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cd253",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Next steps (optional)\n",
    "\n",
    "- Try a different numeric strategy: `\"mean\"` instead of `\"median\"`.\n",
    "- For categorical, try filling with a placeholder like `\"None\"` if it makes sense for the feature.\n",
    "- Create **missingness indicator** columns to remember which values were missing. Example:\n",
    "\n",
    "```python\n",
    "df_with_flags = df_impute_base.copy()\n",
    "for col in df_with_flags.columns:\n",
    "    df_with_flags[col + \"_was_missing\"] = df_with_flags[col].isna().astype(int)\n",
    "```\n",
    "\n",
    "- Explore more advanced imputers later, like `KNNImputer` or model-based methods.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
