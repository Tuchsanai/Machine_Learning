{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0969e2",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§± Sklearn Feature Engineering Pipeline + Grid Search (Ames Housing)\n",
    "\n",
    "This notebook builds a full **feature-engineering pipeline** for the **Ames Housing** dataset using **scikit-learn**.\n",
    "It includes:\n",
    "\n",
    "- Custom outlier cleaning that **calls the provided** `clean_outliers(df_in, method=\"cap\", k=1.5)`\n",
    "- Missing-value handling for **numeric** and **categorical** columns\n",
    "- Encoding and optional scaling\n",
    "- **`Pipeline` + `ColumnTransformer`** integration\n",
    "- **`GridSearchCV`** over outlier parameters, imputation, scaling, and model hyperparameters\n",
    "- Train/validation report with RMSE & \\(R^2\\)\n",
    "\n",
    "> **Note on `method=\"remove\"`**: The original `clean_outliers` function can **drop rows** when `method=\"remove\"`. \n",
    "> Standard scikit-learn `Pipeline` objects expect transformers to **preserve the number of samples** (so that `y` stays aligned).\n",
    "> To keep everything pipeline-safe, our wrapper only uses **`cap`** and **`median`** during grid search.  \n",
    "> If you request `\"remove\"`, we **safely map** it to `\"median\"` internally and print a warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5871fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import warnings\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69930e96",
   "metadata": {},
   "source": [
    "## Provided function: `clean_outliers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Provided function (as requested) ---\n",
    "def clean_outliers(df_in: pd.DataFrame, method: str = \"cap\", k: float = 1.5):\n",
    "    df_clean = df_in.copy()\n",
    "    for col in df_clean.select_dtypes(include=\"number\").columns:\n",
    "        s = df_clean[col]\n",
    "        if s.notna().sum() == 0:\n",
    "            continue\n",
    "        q1, q3 = s.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        low, up = q1 - k * iqr, q3 + k * iqr\n",
    "        if method == \"cap\":\n",
    "            df_clean[col] = s.clip(lower=low, upper=up)\n",
    "        elif method == \"median\":\n",
    "            mask = (s < low) | (s > up)\n",
    "            df_clean.loc[mask, col] = s.median()\n",
    "        elif method == \"remove\":\n",
    "            # WARNING: This drops rows -> not safe inside a sklearn Pipeline that expects X,y alignment.\n",
    "            mask = (s < low) | (s > up)\n",
    "            df_clean = df_clean.loc[~mask]\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e97f1",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline-safe wrapper: `OutlierCleaner`\n",
    "\n",
    "This wrapper **calls `clean_outliers`** but **never changes** the number of rows so the pipeline stays valid.\n",
    "- Supports `method in {\"cap\", \"median\"}` directly.\n",
    "- If `method == \"remove\"`, it **falls back to `\"median\"` and warns** (to keep sample count fixed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe91aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method: str = \"cap\", k: float = 1.5, numeric_only: bool = True):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.numeric_only = numeric_only\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # nothing to learn\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            # Make sure downstream transformers (like ColumnTransformer) still get a DataFrame\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        method = self.method\n",
    "        if method not in {\"cap\", \"median\", \"remove\"}:\n",
    "            raise ValueError(f\"Unsupported method: {method}. Use 'cap', 'median', or 'remove'.\")\n",
    "\n",
    "        # If 'remove' is requested, map to 'median' to avoid changing n_samples\n",
    "        if method == \"remove\":\n",
    "            warnings.warn(\"OutlierCleaner: 'remove' would drop samples; mapping to 'median' for pipeline safety.\")\n",
    "            method = \"median\"\n",
    "\n",
    "        # Optionally restrict to numeric columns only (recommended)\n",
    "        if self.numeric_only:\n",
    "            num_cols = X.select_dtypes(include=\"number\").columns\n",
    "            X_num = X[num_cols]\n",
    "            X_num_clean = clean_outliers(X_num, method=method, k=self.k)\n",
    "            X_clean = X.copy()\n",
    "            X_clean[num_cols] = X_num_clean[num_cols]\n",
    "            return X_clean\n",
    "        else:\n",
    "            return clean_outliers(X, method=method, k=self.k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06454f86",
   "metadata": {},
   "source": [
    "## Load data & define columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load the CSV exactly as requested ---\n",
    "csv_path = \"Ames_Housing_Data.csv\"\n",
    "df       = pd.read_csv(csv_path)\n",
    "\n",
    "# Identify numeric and textual columns:\n",
    "numeric_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "text_columns    = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Target\n",
    "TARGET = \"SalePrice\"\n",
    "assert TARGET in df.columns, f\"{TARGET} not found in DataFrame!\"\n",
    "\n",
    "# Separate features/target\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34a164",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing blocks\n",
    "\n",
    "- **OutlierCleaner** (custom) â†’ numeric columns only\n",
    "- **Numeric pipeline** â†’ imputer (`mean`/`median`) + optional scaler\n",
    "- **Categorical pipeline** â†’ imputer (`most_frequent`/`constant`) + one-hot encoding\n",
    "- Combined via **ColumnTransformer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric pipeline: impute -> (optional) scale\n",
    "numeric_pre = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # grid will try mean/median\n",
    "    (\"scaler\",  StandardScaler(with_mean=True, with_std=True))  # can be toggled via grid\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute -> one-hot\n",
    "categorical_pre = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"missing\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Full preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pre, selector(dtype_include=np.number)),\n",
    "        (\"cat\", categorical_pre, selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True\n",
    ")\n",
    "\n",
    "# Final pipeline: OutlierCleaner -> Preprocessor -> Model\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"outliers\", OutlierCleaner(method=\"cap\", k=1.5, numeric_only=True)),\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d7f90",
   "metadata": {},
   "source": [
    "\n",
    "## Grid Search space\n",
    "\n",
    "We search across:\n",
    "- Outlier cleaning: `method âˆˆ {cap, median}`, `k âˆˆ {1.0, 1.5, 2.0, 3.0}`\n",
    "- Numeric imputer: `mean` vs `median`\n",
    "- Scaling: **enabled** vs **disabled** (by swapping scaler with `passthrough`)\n",
    "- Categorical imputer: `most_frequent` vs `constant`\n",
    "- Model family & hyperparameters:\n",
    "  - **RandomForestRegressor** (n_estimators, max_depth, max_features)\n",
    "  - **Ridge** (alpha)\n",
    "  \n",
    "> Tip: You can expand or reduce the grid to fit your compute budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to toggle scaler in the numeric pipeline\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")  # get DataFrame from transformers for readability\n",
    "\n",
    "param_grid = [\n",
    "    # --- RandomForest branch ---\n",
    "    {\n",
    "        \"outliers__method\": [\"cap\", \"median\"],\n",
    "        \"outliers__k\": [1.0, 1.5, 2.0, 3.0],\n",
    "\n",
    "        \"preprocess__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "        # Toggle scaler: either actual StandardScaler or no scaling\n",
    "        \"preprocess__num__scaler\": [StandardScaler(with_mean=True, with_std=True), \"passthrough\"],\n",
    "\n",
    "        \"preprocess__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "        \"preprocess__cat__imputer__fill_value\": [\"missing\"],  # used when strategy='constant'\n",
    "\n",
    "        \"model\": [RandomForestRegressor(random_state=RANDOM_STATE)],\n",
    "        \"model__n_estimators\": [300, 600],\n",
    "        \"model__max_depth\": [None, 12, 20],\n",
    "        \"model__max_features\": [\"sqrt\", \"log2\", 0.6, 1.0],\n",
    "    },\n",
    "    # --- Ridge branch (linear baseline) ---\n",
    "    {\n",
    "        \"outliers__method\": [\"cap\", \"median\"],\n",
    "        \"outliers__k\": [1.0, 1.5, 2.0, 3.0],\n",
    "\n",
    "        \"preprocess__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "        \"preprocess__num__scaler\": [StandardScaler(with_mean=True, with_std=True), \"passthrough\"],\n",
    "\n",
    "        \"preprocess__cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "        \"preprocess__cat__imputer__fill_value\": [\"missing\"],\n",
    "\n",
    "        \"model\": [Ridge(random_state=RANDOM_STATE) if hasattr(Ridge(), \"random_state\") else Ridge()],\n",
    "        \"model__alpha\": [0.1, 1.0, 10.0, 100.0],\n",
    "    },\n",
    "]\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    verbose=1\n",
    ")\n",
    "search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07fb1a",
   "metadata": {},
   "source": [
    "\n",
    "> **Optional:** The full grid may take time. For a quick smoke test, reduce the grid sizes before running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aaf3c0",
   "metadata": {},
   "source": [
    "## Fit Grid Search & Evaluate on Holdout Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217615dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run the search (may take several minutes depending on CPU/RAM) ---\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\")\n",
    "print(search.best_params_)\n",
    "print(\"\\nCV best score (neg RMSE):\", search.best_score_)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest RMSE:\", rmse)\n",
    "print(\"Test R^2:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07667cec",
   "metadata": {},
   "source": [
    "## Inspect engineered feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be52abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After fitting, you can inspect the output feature names from the preprocessor\n",
    "final_pre = best_model.named_steps[\"preprocess\"]\n",
    "feature_names = []\n",
    "# numeric\n",
    "num_features = final_pre.named_transformers_[\"num\"].get_feature_names_out()\n",
    "feature_names.extend(num_features.tolist())\n",
    "# categorical\n",
    "cat_features = final_pre.named_transformers_[\"cat\"].named_steps[\"ohe\"].get_feature_names_out()\n",
    "feature_names.extend(cat_features.tolist())\n",
    "\n",
    "print(f\"Total engineered features: {len(feature_names)}\")\n",
    "pd.Series(feature_names).head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bca740",
   "metadata": {},
   "source": [
    "## Quick report dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "cols = [\n",
    "    'rank_test_score','mean_test_score','std_test_score',\n",
    "    'mean_train_score','std_train_score','param_outliers__method','param_outliers__k',\n",
    "    'param_preprocess__num__imputer__strategy','param_preprocess__num__scaler',\n",
    "    'param_preprocess__cat__imputer__strategy','param_model'\n",
    "]\n",
    "display(results[cols].sort_values('rank_test_score').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ae19c",
   "metadata": {},
   "source": [
    "\n",
    "## Tips to extend\n",
    "\n",
    "- Add more models (e.g., `GradientBoostingRegressor`, `HistGradientBoostingRegressor`).\n",
    "- Add a **PowerTransformer** for skewed numeric features.\n",
    "- Replace the `\"remove\"` behavior by *masking outliers as `NaN`*, letting the imputer fill them â€” this preserves sample count and stays pipeline-safe.\n",
    "- Use **`HalvingGridSearchCV`** or **`RandomizedSearchCV`** for speed.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}