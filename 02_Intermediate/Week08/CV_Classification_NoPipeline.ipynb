{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd109120",
   "metadata": {},
   "source": [
    "# Cross-Validation for Classification (scikit-learn) — No Pipeline\n",
    "\n",
    "**Level:** Entry → Middle  \n",
    "**Goal:** Learn how to do K-Fold / Stratified K-Fold cross-validation, compare models, and avoid data leakage — **without using `Pipeline`**.\n",
    "\n",
    "### What you'll practice\n",
    "- K-Fold vs. **Stratified** K-Fold\n",
    "- Baseline with `DummyClassifier`\n",
    "- Manual cross-validation loop (so you see what's happening under the hood)\n",
    "- Proper scaling *inside* each fold (no leakage) — still **no Pipeline**\n",
    "- Simple hyperparameter search with CV\n",
    "- Final evaluation on a hold-out test set\n",
    "\n",
    "> Dataset: `Breast Cancer Wisconsin (Diagnostic)` from scikit-learn (binary classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e387318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import __version__ as sk_version\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('Versions -> numpy:', np.__version__, '| pandas:', pd.__version__, '| scikit-learn:', sk_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d1844",
   "metadata": {},
   "source": [
    "## 1) Load the dataset and create a hold-out test set\n",
    "\n",
    "We'll keep a separate **test set** to evaluate the final model after we finish cross-validation and selection on the training split only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as (X, y)\n",
    "data = load_breast_cancer()\n",
    "X_full = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_full = pd.Series(data.target, name='target')\n",
    "\n",
    "# Hold-out split: we'll only use the training part for CV.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.value_counts().to_dict(), y_test.value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2af39",
   "metadata": {},
   "source": [
    "## 2) Baseline with `DummyClassifier` + Stratified K-Fold\n",
    "\n",
    "Stratification keeps class balance similar across folds. Baselines help us judge whether a real model is doing better than \"always predict the majority class\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "baseline = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
    "accs = cross_val_score(baseline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print('Baseline accuracy per fold:', accs)\n",
    "print('Baseline accuracy mean±std: %.3f ± %.3f' % (accs.mean(), accs.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c25a6",
   "metadata": {},
   "source": [
    "## 3) Quick model: Decision Tree with Stratified K-Fold\n",
    "\n",
    "Trees don't require feature scaling, so we can use `cross_val_score` directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "accs_tree = cross_val_score(tree, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print('DecisionTree accuracy per fold:', accs_tree)\n",
    "print('DecisionTree mean±std: %.3f ± %.3f' % (accs_tree.mean(), accs_tree.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8299",
   "metadata": {},
   "source": [
    "## 4) Logistic Regression — Manual CV loop with **scaling inside each fold**\n",
    "\n",
    "To avoid leakage, we must fit the scaler **only on the training fold**, then transform the validation fold.\n",
    "We will code the CV loop ourselves to make this explicit (still **no Pipeline**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e491847",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_lr, f1s_lr = [], []\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X_train, y_train), start=1):\n",
    "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
    "\n",
    "    # scale inside the fold\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = scaler.fit_transform(X_tr)\n",
    "    X_va_s = scaler.transform(X_va)\n",
    "\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    lr.fit(X_tr_s, y_tr)\n",
    "    pred = lr.predict(X_va_s)\n",
    "\n",
    "    accs_lr.append(accuracy_score(y_va, pred))\n",
    "    f1s_lr.append(f1_score(y_va, pred))\n",
    "\n",
    "    print(f'Fold {fold} -> acc={accs_lr[-1]:.3f}, f1={f1s_lr[-1]:.3f}')\n",
    "\n",
    "print('\\nLogReg accuracy mean±std: %.3f ± %.3f' % (np.mean(accs_lr), np.std(accs_lr)))\n",
    "print('LogReg F1 mean±std: %.3f ± %.3f' % (np.mean(f1s_lr), np.std(f1s_lr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4a057",
   "metadata": {},
   "source": [
    "## 5) (Optional) K-Fold vs. Stratified K-Fold — class balance check\n",
    "\n",
    "Let's see the positive-class proportion in each fold for both strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4279c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_pos_rate(splitter, y):\n",
    "    rates = []\n",
    "    for tr, va in splitter.split(np.zeros(len(y)), y):\n",
    "        rates.append(y.iloc[va].mean())\n",
    "    return np.array(rates)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rates_strat = fold_pos_rate(cv, y_train)\n",
    "rates_kfold = fold_pos_rate(kfold, y_train)\n",
    "\n",
    "print('StratifiedKFold pos rate per fold:', np.round(rates_strat, 3))\n",
    "print('KFold pos rate per fold:', np.round(rates_kfold, 3))\n",
    "\n",
    "# simple bar chart to visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "x = np.arange(1, 6)\n",
    "plt.plot(x, rates_strat, marker='o', label='StratifiedKFold')\n",
    "plt.plot(x, rates_kfold, marker='o', label='KFold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Positive class proportion')\n",
    "plt.title('Class balance per fold')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7985823",
   "metadata": {},
   "source": [
    "## 6) Manual hyperparameter search with CV (Decision Tree `max_depth`)\n",
    "\n",
    "We'll loop over a few depths, compute CV accuracy, and pick the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7486bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(2, 11))\n",
    "results = []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    results.append((d, scores.mean(), scores.std()))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['max_depth', 'mean_acc', 'std_acc']).sort_values('mean_acc', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "best_depth = int(results_df.iloc[0]['max_depth'])\n",
    "print('\\nBest max_depth:', best_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a4a50",
   "metadata": {},
   "source": [
    "## 7) Train the selected model on the full training set and evaluate on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = DecisionTreeClassifier(max_depth=best_depth, random_state=RANDOM_STATE)\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "test_pred = best_tree.predict(X_test)\n",
    "\n",
    "print('Test accuracy:', accuracy_score(y_test, test_pred))\n",
    "print('Test precision:', precision_score(y_test, test_pred))\n",
    "print('Test recall:', recall_score(y_test, test_pred))\n",
    "print('Test F1:', f1_score(y_test, test_pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "print('Confusion matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50f359",
   "metadata": {},
   "source": [
    "## 8) Exercises\n",
    "1. Try `RepeatedStratifiedKFold` to reduce variance. Compare means and standard deviations.\n",
    "2. Add another model (e.g., `KNeighborsClassifier`) and (**inside each fold**) standardize the features before fitting.\n",
    "3. Change scoring metrics to `'f1'` or `'roc_auc'` and compare rankings.\n",
    "4. Create your own manual CV loop for `DecisionTreeClassifier` (no `cross_val_score`) to compute accuracy per fold.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
